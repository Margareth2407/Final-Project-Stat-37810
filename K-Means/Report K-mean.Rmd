---
title: "Report K mean"
author: "Margarita Orlova, Kenneth Ruiter, Patrick Walker"
date: "November 3, 2018"
output: pdf_document
---
#K-means classifier

In the code below we cluster data from the wine dataset, using a k-means method.
Our function returns a list with 2 objects: the means of the clusters and the results of our clustering. We receive clusters with a total of 47, 62 and 69 observations. Note that the k-means function built in R produces the same result, meaning that our code is working.

```{r, message=FALSE}
library(rattle) #import data
library(fpc) #import plot
data(wine)

mywine <- as.matrix(wine[,-1]) # remove the existing clustering

cluster <- function(data){ #declare function
  nrows <- length(data[,1]) #create variable with number of rows of data
  ncols <- length(data[1,]) #create variable with number of columns of data
  iterations <- 20
  set.seed(1) 
  means <- data[sample(1:nrows,3),] #select rows from the original data set to serve
    #as the initial means/centers
  for(i in 1:iterations){ #for loop for each of the iterations of reassigning the means
    distances <- matrix(rep(0,3*nrows),nrow = nrows) #distances matrix initialized
    mindistances <- matrix(rep(0, 2*nrows), nrow = nrows) #mindistances matrix initialized
    for(j in 1:nrows){ #for each row
      for(k in 1:3){ #for each cluster
        distances[j,k] <- sum((data[j,]-means[k,])^2) 
        #finds the difference between the row j and the mean ascribed to cluster K
      }
      mindistances[j,] <- c(min(distances[j,]), which.min(distances[j,])) 
      #finds the minimum distance for each row amongst the three clusters and assigns
      #it to mindistances in the first column and the second column has the cluster
    }
    for(l in 1:3){ #for each cluster
      datarows <- data[mindistances[,2]==l,] #selects the data corresponding to
        #each of the clusters
      means[l,] <- colMeans(data[mindistances[,2]==l,])
      #the above line takes the mean of each column and 
      #stores them as the new means vector
    }
  }
  return(list(means, mindistances)) #returns means and mindistances
}
results <- cluster(mywine) 
results[[1]][,]
sum(results[[2]][,2] == 1)
sum(results[[2]][,2] == 2)
sum(results[[2]][,2] == 3)
```

```{r}
kmeans(wine[,-1],3,algorith="Lloyd")  #running R k-means classifier as comparison
```


Then we visualize our clusters to check how well are they separated. And as we see from the plot, observations are grouped adequate, however there is no distinct border between clusters.

```{r}
plotcluster(mywine, results[[2]][,2]) #plots clusters
```


Later, we have to quantify the results of k-mean clustering. In order to do that we run following code:

```{r, message=FALSE}
require(data.table)
wine_cluster <- cbind(wine,results[[2]][,2]) #add variable with  cluster to wine dataset
colnames(wine_cluster)[15] <- "clusters" #name new variable
type_1 <-setkey(data.table((wine_cluster[wine_cluster$Type == 1,2:14]))) 
  #subgroup observation with type 1 
type_2 <-setkey(data.table((wine_cluster[wine_cluster$Type == 2,2:14]))) 
  #subgroup observation with type 2 
type_3 <-setkey(data.table((wine_cluster[wine_cluster$Type == 3,2:14]))) 
  #subgroup observation with type 3 
cluster_1 <-setkey(data.table((wine_cluster[wine_cluster$clusters == 1,2:14]))) 
  #subgroup observation that are classified as claster 1 
cluster_2 <-setkey(data.table((wine_cluster[wine_cluster$clusters == 2,2:14]))) 
  #subgroup observation that are classified as claster 2
cluster_3 <-setkey(data.table((wine_cluster[wine_cluster$clusters == 3,2:14]))) 
  #subgroup observation that are classified as claster 3
cluster1_type1<-na.omit(cluster_1[type_1,which=TRUE]) 
  #rows that in cluster 1 and originally typed as 1
cluster1_type2<-na.omit(cluster_1[type_2,which=TRUE]) 
  #rows that in cluster 1 and originally typed as 2
cluster1_type3<-na.omit(cluster_1[type_3,which=TRUE]) 
  #rows that in cluster 1 and originally typed as 3
cluster2_type1<-na.omit(cluster_2[type_1,which=TRUE]) 
  #rows that in cluster 2 and originally typed as 1
cluster2_type2<-na.omit(cluster_2[type_2,which=TRUE]) 
  #rows that in cluster 2 and originally typed as 2
cluster2_type3<-na.omit(cluster_2[type_3,which=TRUE]) 
  #rows that in cluster 2 and originally typed as 3
cluster3_type1<-na.omit(cluster_3[type_1,which=TRUE]) 
  #rows that in cluster 3 and originally typed as 1
cluster3_type2<-na.omit(cluster_3[type_2,which=TRUE]) 
  #rows that in cluster 3 and originally typed as 2
cluster3_type3<-na.omit(cluster_3[type_3,which=TRUE]) 
  #rows that in cluster 3 and originally typed as 3

method<-c(length(cluster1_type1),length(cluster1_type2),length(cluster1_type3)
          ,length(cluster2_type1),length(cluster2_type2),length(cluster2_type3)
          ,length(cluster3_type1),length(cluster3_type2),length(cluster3_type3))
  #count rows in groups cluster-type
matrix<-matrix(method,nrow = 3,ncol = 3, byrow=TRUE) #create matrix 3 by 3 
colnames(matrix) <- c("Type 1","Type 2","Type 3") #set column names
rownames(matrix) <- c("Cluster 1","Cluster 2","Cluster 3") #set row names
matrix
```

The output of following code is a 3 by 3 matrix that can be interpreted as follows: The intersection of row Cluster 1 and column Type 1 is the amount of observation in cluster 1 which are also type 1 in the original wine file. The sum of each row must be equal to the size of the clusters (47,62,69) and it is exactly what we observe.

As we see, cluster 1 mostly consist of type 1 wine, with only one type 2 wine. Cluster 2 can be interpreted as mostly type 2 wines, with 19 misgrouped type 3 wines. Cluster 3 is not that easy to interpret as it includes all types of wine. However, we can say that it is type 3 wine cluster.



##K-means for scaled data

Using the same code, we use scaled data this time. We will not define our function again and use code above. The function returns clusters with 51, 61 and 66 observations. As we see from the plot, scaling the data improves clustering a lot. After scaling observation can be distinctly grouped with clear borders between clusters. 

```{r}
data(wine) #scale data
set.seed(0)
mywine_scaled <- as.matrix(scale(wine[,-1])) # remove the existing clustering

results <- cluster(mywine_scaled)

results[[1]][,]
sum(results[[2]][,2] == 1)
sum(results[[2]][,2] == 2)
sum(results[[2]][,2] == 3)

plotcluster(mywine, results[[2]][,2]) #plots clusters
```

Our method for checking quality of the clustering also shows improvement over unscaled data.  For example, cluster 2 fully consist of type 2 wine. Cluster 1 became assosiated with type 3 wine with only 3 observations of type 2. Cluster 2 attributes type 1 wine with only 2 wrongly clustered obseration. 
Thus, after scaling the data, the k-means cluster algorithm wrongly classifies only 5 rows which is 3% of the total dataset.
 
```{r, echo=FALSE}
wine_cluster_scaled <- cbind(wine,results[[2]][,2])
colnames(wine_cluster_scaled)[15] <- "clusters"
type_1 <-setkey(data.table((wine_cluster_scaled[wine_cluster_scaled$Type == 1,2:14])))
type_2 <-setkey(data.table((wine_cluster_scaled[wine_cluster_scaled$Type == 2,2:14])))
type_3 <-setkey(data.table((wine_cluster_scaled[wine_cluster_scaled$Type == 3,2:14])))
cluster_1 <-setkey(data.table((wine_cluster_scaled[wine_cluster_scaled$clusters == 1,2:14])))
cluster_2 <-setkey(data.table((wine_cluster_scaled[wine_cluster_scaled$clusters == 2,2:14])))
cluster_3 <-setkey(data.table((wine_cluster_scaled[wine_cluster_scaled$clusters == 3,2:14])))
cluster1_type1<-na.omit(cluster_1[type_1,which=TRUE])
cluster1_type2<-na.omit(cluster_1[type_2,which=TRUE])
cluster1_type3<-na.omit(cluster_1[type_3,which=TRUE])
cluster2_type1<-na.omit(cluster_2[type_1,which=TRUE])
cluster2_type2<-na.omit(cluster_2[type_2,which=TRUE])
cluster2_type3<-na.omit(cluster_2[type_3,which=TRUE])
cluster3_type1<-na.omit(cluster_3[type_1,which=TRUE])
cluster3_type2<-na.omit(cluster_3[type_2,which=TRUE])
cluster3_type3<-na.omit(cluster_3[type_3,which=TRUE])

method<-c(length(cluster1_type1),length(cluster1_type2),length(cluster1_type3),length(cluster2_type1),length(cluster2_type2),length(cluster2_type3),length(cluster3_type1),length(cluster3_type2),length(cluster3_type3))
matrix<-matrix(method,nrow = 3,ncol = 3, byrow=TRUE)
colnames(matrix) <- c("Type 1","Type 2","Type 3")
rownames(matrix) <- c("Cluster 1","Cluster 2","Cluster 3")
matrix

```





##K-mean for iris dataset
In the following section we will run our previously defined function on the iris dataset and only new code will be commented, since we are running the same procedure.

Our function clusters the dataset into groups of 39, 50 and 61 observations. From the plot we can observe that cluster 1 is separated from other data, but clusters 2 and 3 are hard to distinguish. 

```{r}
data(iris)
iris <- as.matrix(iris[,1:4]) # remove column with species
set.seed(421)

results <- cluster(iris) 

results[[1]][,]
sum(results[[2]][,2] == 1)
sum(results[[2]][,2] == 2)
sum(results[[2]][,2] == 3)
plotcluster(iris, results[[2]][,2]) 

```

We can also notice that from this dataset, the k-means method clusters species 1 properly, but has more mistakes in grouping species 2 and 3 because 19 out of 150 observations were misgrouped (12,6% from total dataset).


```{r}
data(iris)

#Chaning properties of the data set so that our comparison method can be applied
iris <- as.matrix(iris) 
iris[iris=="setosa"] <- 1 #recode variable species 
iris[iris=="versicolor"] <- 2 #recode variable species
iris[iris=="virginica"] <- 3 #recode variable species
iris_clusters <- cbind(results[[2]][,2],iris) #add first column with cluster
colnames(iris_clusters)[1] <- "clusters"

iris_clusters<-as.data.frame(iris_clusters)
```

```{r, echo=FALSE}
require(data.table)
species_1 <-setkey(data.table((iris_clusters[iris_clusters$Species == 1,2:5])))
species_2 <-setkey(data.table((iris_clusters[iris_clusters$Species == 2,2:5])))
species_3 <-setkey(data.table((iris_clusters[iris_clusters$Species == 3,2:5])))
cluster_1 <-setkey(data.table((iris_clusters[iris_clusters$clusters == 1,2:5])))
cluster_2 <-setkey(data.table((iris_clusters[iris_clusters$clusters == 2,2:5])))
cluster_3 <-setkey(data.table((iris_clusters[iris_clusters$clusters == 3,2:5])))
cluster1_species_1<-na.omit(cluster_1[species_1,which=TRUE])
cluster1_species_2<-na.omit(cluster_1[species_2,which=TRUE])
cluster1_species_3<-na.omit(cluster_1[species_3,which=TRUE])
cluster2_species_1<-na.omit(cluster_2[species_1,which=TRUE])
cluster2_species_2<-na.omit(cluster_2[species_2,which=TRUE])
cluster2_species_3<-na.omit(cluster_2[species_3,which=TRUE])
cluster3_species_1<-na.omit(cluster_3[species_1,which=TRUE])
cluster3_species_2<-na.omit(cluster_3[species_2,which=TRUE])
cluster3_species_3<-na.omit(cluster_3[species_3,which=TRUE])

method<-c(length(cluster1_species_1),length(cluster1_species_2),length(cluster1_species_3),length(cluster2_species_1),length(cluster2_species_2),length(cluster2_species_3),length(cluster3_species_1),length(cluster3_species_2),length(cluster3_species_3))
matrix<-matrix(method,nrow = 3,ncol = 3, byrow=TRUE)
colnames(matrix) <- c("Species 1","Species 2","Species 3")
rownames(matrix) <- c("Cluster 1","Cluster 2","Cluster 3")
matrix

```


##K-means for iris scaled data

In the code below we first scale iris data in order to improve results of k-mean clustering.
We receive groups of 50, 56, 44 observations. From the plot we can observe that again cluster 1 is separated from the majority of the data and thus clustered properly, but clusters 2 and 3 are still hard to distinguish. 

```{r}
data(iris)
iris <- as.matrix(iris[,1:4]) # remove column with species
iris_scaled <- as.matrix(scale(iris)) #scale data
set.seed(421)

results <- cluster(iris_scaled) 

results[[1]][,]
sum(results[[2]][,2] == 1)
sum(results[[2]][,2] == 2)
sum(results[[2]][,2] == 3)
plotcluster(iris, results[[2]][,2]) 
```


Corresponding with the result of the unscaled data, cluster 1 assosiated with species 1 is matched correctly, however scaling does not improve clustering for species 2 and 3. In fact, instead of 19 misgrouped observation we receive 30 (16% from dataset). Thus, we can conclude that scaling data does not always improve results of clustering and in some cases can negatively affect it.

```{r, echo=FALSE}
data(iris)
iris <- as.matrix(iris)
iris_scaled <- cbind(iris_scaled,iris[,5]) #add species column 
iris_scaled[iris_scaled=="setosa"] <- 1 #recode variable species 
iris_scaled[iris_scaled=="versicolor"] <- 2 #recode variable species
iris_scaled[iris_scaled=="virginica"] <- 3 #recode variable species

iris_scaled <- cbind(results[[2]][,2],iris_scaled) #add first column with cluster
colnames(iris_scaled)[1] <- "clusters"
colnames(iris_scaled)[6] <- "Species"

iris_scaled<-as.data.frame(iris_scaled)

require(data.table)
species_1 <-setkey(data.table((iris_scaled[iris_scaled$Species == 1,2:5])))
species_2 <-setkey(data.table((iris_scaled[iris_scaled$Species == 2,2:5])))
species_3 <-setkey(data.table((iris_scaled[iris_scaled$Species == 3,2:5])))
cluster_1 <-setkey(data.table((iris_scaled[iris_scaled$clusters == 1,2:5])))
cluster_2 <-setkey(data.table((iris_scaled[iris_scaled$clusters == 2,2:5])))
cluster_3 <-setkey(data.table((iris_scaled[iris_scaled$clusters == 3,2:5])))


cluster1_species_1<-na.omit(cluster_1[species_1,which=TRUE])
cluster1_species_2<-na.omit(cluster_1[species_2,which=TRUE])
cluster1_species_3<-na.omit(cluster_1[species_3,which=TRUE])
cluster2_species_1<-na.omit(cluster_2[species_1,which=TRUE])
cluster2_species_2<-na.omit(cluster_2[species_2,which=TRUE])
cluster2_species_3<-na.omit(cluster_2[species_3,which=TRUE])
cluster3_species_1<-na.omit(cluster_3[species_1,which=TRUE])
cluster3_species_2<-na.omit(cluster_3[species_2,which=TRUE])
cluster3_species_3<-na.omit(cluster_3[species_3,which=TRUE])

method<-c(length(cluster1_species_1),length(cluster1_species_2),length(cluster1_species_3),length(cluster2_species_1),length(cluster2_species_2),length(cluster2_species_3),length(cluster3_species_1),length(cluster3_species_2),length(cluster3_species_3))
matrix<-matrix(method,nrow = 3,ncol = 3, byrow=TRUE)
colnames(matrix) <- c("Species 1","Species 2","Species 3")
rownames(matrix) <- c("Cluster 1","Cluster 2","Cluster 3")
matrix

```
